{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtErKPD9D2LqibtmNvXhgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham6907/AI-Basics/blob/main/Gen_AI_Important_Tools_and_Processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gen AI: Important Tools and Processes**"
      ],
      "metadata": {
        "id": "DxecO8VgR_NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Retrieval-Augmented Generation (RAG)**"
      ],
      "metadata": {
        "id": "JHJYvTRjS0rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts."
      ],
      "metadata": {
        "id": "VkvPTl6NTU_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is Retrieval-Augmented Generation important?**\n",
        "\n",
        "LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\n",
        "\n",
        "Known challenges of LLMs include:\n",
        "*   Presenting false information when it does not have the answer.\n",
        "*   Presenting out-of-date or generic information when the user expects a specific, current response.\n",
        "*   Creating a response from non-authoritative sources.\n",
        "*   Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\n",
        "\n",
        "You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\n",
        "\n",
        "RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response."
      ],
      "metadata": {
        "id": "fGmzHlkgTKKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are the benefits of Retrieval-Augmented Generation?**\n",
        "\n",
        "RAG technology brings several benefits to an organization's generative AI efforts.\n",
        "\n",
        "**Cost-effective implementation**\n",
        "\n",
        "Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\n",
        "\n",
        "**Current information**\n",
        "\n",
        "Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\n",
        "\n",
        "**Enhanced user trust**\n",
        "\n",
        "RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\n",
        "\n",
        "**More developer control**\n",
        "\n",
        "With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications."
      ],
      "metadata": {
        "id": "BFJgbCXUVO8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does Retrieval-Augmented Generation work?**\n",
        "\n",
        "Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.\n",
        "\n",
        "**Create external data**\n",
        "\n",
        "The new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.\n",
        "\n",
        "**Retrieve relevant information**\n",
        "\n",
        "The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\n",
        "\n",
        "**Augment the LLM prompt**\n",
        "\n",
        "Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\n",
        "\n",
        "**Update external data**\n",
        "\n",
        "The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used."
      ],
      "metadata": {
        "id": "AoBw5ZcXV3vF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the difference between Retrieval-Augmented Generation and semantic search?**\n",
        "\n",
        "Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality.\n",
        "\n",
        "Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM.\n",
        "\n",
        "Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don't have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload."
      ],
      "metadata": {
        "id": "grz0ekULWLDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can AWS support your Retrieval-Augmented Generation requirements?**\n",
        "\n",
        "**Amazon Bedrock** is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically.\n",
        "\n",
        "For organizations managing their own RAG, **Amazon Kendra** is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can:\n",
        "\n",
        "*   Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\n",
        "*   Use pre-built connectors to popular data technologies like **Amazon Simple Storage Service**, SharePoint, Confluence, and other websites.\n",
        "*   Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\n",
        "*   Filter responses based on those documents that the end-user permissions allow.\n",
        "\n",
        "Amazon also offers options for organizations who want to build more custom generative AI solutions. **Amazon SageMaker** JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples."
      ],
      "metadata": {
        "id": "2WJeRkN6W9qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vector Stores**"
      ],
      "metadata": {
        "id": "BwQ8UPtzX7wy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time, embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you."
      ],
      "metadata": {
        "id": "3wU7ewllY46W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine.\n",
        "*   chroma\n",
        "*   FAISS\n",
        "*   Lance"
      ],
      "metadata": {
        "id": "nIoWUgLtY-Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Text Embedding Models**"
      ],
      "metadata": {
        "id": "-wY8kwDRbqfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n",
        "\n",
        "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
        "\n",
        "The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself)."
      ],
      "metadata": {
        "id": "Af4oKUXZbwoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain**"
      ],
      "metadata": {
        "id": "OWbZ1VFnb3Si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. It allows AI developers to develop applications based on the combined Large Language Models (LLMs) such as GPT-4 with external sources of computation and data. This framework comes with a package for both Python and JavaScript.\n",
        "\n",
        "LangChain follows a general pipeline where a user asks a question to the language model where the vector representation of the question is used to do a similarity search in the vector database and the relevant information is fetched from the vector database and the response is later fed to the language model. further, the language model generates an answer or takes an action."
      ],
      "metadata": {
        "id": "wn8EIjnLH_7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications of LangChain**\n",
        "\n",
        "LangChain is a powerful tool that can be used to build a wide range of LLM-powered applications. It is simple to use and has a large user and contributor community.\n",
        "\n",
        "*   Document analysis and summarization\n",
        "*   Chatbots: LangChain can be used to build chatbots that interact with users naturally. For example, LangChain can be used to build a chatbot that can answer client questions, provide customer assistance, and even arrange appointments.\n",
        "*   Code analysis: LangChain can be used to analyse code and find potential bugs or security flaws.\n",
        "*   Answering questions using sources: LangChain can be used to answer questions using a variety of sources, including text, code, and data. For example, LangChain can be used to answer questions about a specific topic by searching through a variety of sources, such as Wikipedia, news articles, and code repositories.\n",
        "*   Data augmentation: LangChain can be used to augment data by generating new data that is similar to existing data. For example, LangChain can be used to generate new text data that is similar to existing text data. This can be useful for training machine learning models or for creating new datasets.\n",
        "*   Text classification: LangChain can be used for text classifications and sentiment analysis with the text input data\n",
        "*   Text summarization: LangChain can be used to summarize the text in the specified number of words or sentences.\n",
        "*   Machine translation: LangChain can be used to translate the input text data into different languages."
      ],
      "metadata": {
        "id": "fMEaWSV0IkaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangChain Key Concepts:**\n",
        "\n",
        "The main properties of LangChain Framework are :\n",
        "\n",
        "*   Components: Components are modular building blocks that are ready and easy to use to build powerful applications. Components include LLM Wrappers, Prompt Template and Indexes for relevant information retrieval.\n",
        "*   Chains: Chains allow us to combine multiple components together to solve a specific task. Chains make it easy for the implementation of complex applications by making it more modular and simple to debug and maintain.\n",
        "*   Agents: Agents allow LLMs to interact with their environment. For example, using an external API to perform a specific action."
      ],
      "metadata": {
        "id": "COE4FhrMJunB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mF93uvqTKB8j"
      }
    }
  ]
}