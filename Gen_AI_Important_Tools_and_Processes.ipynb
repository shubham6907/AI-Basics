{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPq3k0Ecg3kON3hbN6xbLbX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham6907/AI-Basics/blob/main/Gen_AI_Important_Tools_and_Processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gen AI: Important Tools and Processes**"
      ],
      "metadata": {
        "id": "DxecO8VgR_NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Retrieval-Augmented Generation (RAG)**"
      ],
      "metadata": {
        "id": "JHJYvTRjS0rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts."
      ],
      "metadata": {
        "id": "VkvPTl6NTU_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is Retrieval-Augmented Generation important?**\n",
        "\n",
        "LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\n",
        "\n",
        "Known challenges of LLMs include:\n",
        "*   Presenting false information when it does not have the answer.\n",
        "*   Presenting out-of-date or generic information when the user expects a specific, current response.\n",
        "*   Creating a response from non-authoritative sources.\n",
        "*   Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\n",
        "\n",
        "You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\n",
        "\n",
        "RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response."
      ],
      "metadata": {
        "id": "fGmzHlkgTKKw"
      }
    }
  ]
}