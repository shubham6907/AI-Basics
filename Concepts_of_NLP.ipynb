{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKQ2r8p929T4qwKxC6c1CC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham6907/AI-Basics/blob/main/Concepts_of_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Concepts of Natural Language Processing (NLP)**\n",
        "\n",
        "NLP stands for Natural Language Processing. It is the branch of Artificial Intelligence that gives the ability to machine understand and process human languages. Human languages can be in the form of text or audio format.\n",
        "\n",
        "Natural Language Processing (NLP) is a very important component for its vast applications in various industries/sectors. For a human it’s pretty easy to understand the language but machines are not capable enough to recognize it easily. NLP is the technique that enables the machines to interpret and to understand the way humans communicate.\n",
        "\n",
        "At present social media is a golden data mine for natural languages, be it any type of reviews from any online sites (Amazon, Google, etc.), or simply posts from Twitter, Facebook, LinkedIn, or emails. The business use cases (Classifications, Text Summarization, Triaging, Interactive voice responses (IVR), Language translation, Chatbots) might be different in each sector but NLP defines the core underlying solution of these use cases.\n",
        "\n",
        "Natural languages are a free form of text which means it is very much unstructured in nature. So, cleaning and preparing the data to extract the features are very important for the NLP journey while developing any model.\n",
        "\n",
        "A) Data Cleaning\n",
        "\n",
        "B) Tokenization\n",
        "\n",
        "C) Vectorization/Word Embedding\n",
        "\n",
        "D) Model Development"
      ],
      "metadata": {
        "id": "wpQHpTMgXTGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A) Data Cleaning**\n",
        "As mentioned above, data cleaning is basic but very important step in NLP.\n",
        "1. **Remove stopwords:** There are a few words which are very commonly used when humans interact, but these words don’t make any sense or add any extra value. Additionally, there might be few words which are not required for the business case given in hand. So, these words need to be deleted from the data.\n",
        "\n",
        "2. **Make lower case:** This is required to make all the words in lowercase just to maintain the uniformity.\n",
        "\n",
        "3. **Lemmatization:** This helps to reduce the words into a single form. In contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
        "\n",
        "4. **Stemming:** This helps to reduce the words into their root form.\n",
        "There are 2 more types of stemming apart from PorterStemmer. Those are Lancaster Stemming and Snowball. Snowball is an improvement over Porter stemming. Stemming is a method in text processing that eliminates prefixes and suffixes from words, transforming them into their fundamental or root form, The main objective of stemming is to streamline and standardize words, enhancing the effectiveness of the natural language processing tasks. Simplifying words to their most basic form is called stemming, and it is made easier by stemmers or stemming algorithms. For example, “chocolates” becomes “chocolate” and “retrieval” becomes “retrieve.” This is crucial for pipelines for natural language processing, which use tokenized words that are acquired from the first stage of dissecting a document into its constituent words. Stemming in natural language processing reduces words to their base or root form, aiding in text normalization for easier processing. This technique is crucial in tasks like text classification, information retrieval, and text summarization. While beneficial, stemming has drawbacks, including potential impacts on text readability and occasional inaccuracies in determining the correct root form of a word.\n",
        "\n",
        "5. **Removal of regular expressions:** Regular expression helps to identify and to get rid of different patterns which are not required in the text.\n",
        "\n",
        "6. **Parts-of-Speech (POS) tagging:** This helps to identify the parts of speech. Based on the use case one can keep or remove some of them.\n",
        "\n",
        "7. **Named-Entity-Recognition (NER):** This helps to identify and categorize the different groups which includes names, places, currency etc."
      ],
      "metadata": {
        "id": "8Gk4qrbUXjMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **B) Tokenization**\n",
        "This is one of the common practices while working on text data. This helps to split a phrase, sentence, or paragraph into small units like words or terms. Each unit is called a token. There are different types of tokenization. Below are different ways to tokenize the text.\n",
        "\n",
        "1. **Tokenization using split () function:** Returns list of strings after breaking the given string by the specified separator. By default, a separator is a space.\n",
        "\n",
        "2. **Tokenization using Regular expression:** Returns list based on the regular expressions.\n",
        "\n",
        "3. **Tokenization using NLTK:** There are different types of tokenizers under NLTK package like word tokenizer (word_tokenize), regex tokenizer (RegexpTokenizer), whitespace tokenizer (WhitespaceTokenizer) etc.\n",
        "\n",
        "Further we can use these tokenized forms to count the number of words in a text or frequency of the words in a text."
      ],
      "metadata": {
        "id": "7QSSxJAxYrN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C) Vectorization/Word Embedding**\n",
        "\n",
        "Once cleaning and tokenization is done, extracting features from the clean data is very important as the machine doesn’t understand the words but numbers. Vectorization helps to map the words to a vector of real numbers, which further helps into predictions. This helps to extract the important features. Below are few techniques used for the purpose:\n",
        "\n",
        "1. **CountVec:** Count number of times a particular word appears in the document. CountVectorizer helps to get this count.\n",
        "\n",
        "2. **TF-IDF:** Term frequency Inverse document frequency (TF-IDF) provides an overall weightage of a word in the document. TfidfVectorizer helps to get this weighted score.\n",
        "\n",
        "Having said all these, Bag of words or TF-IDF (mainly) is vastly used till now and very much integrated part of day-to-day NLP problems."
      ],
      "metadata": {
        "id": "YVSJRjq9ZTEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **D) Model Development**\n",
        "Here comes the final part! Finally we have a count based or TF-IDF matrix and the dependent variable (label) to develop the model.\n",
        "\n",
        "One can use any of the classification models like logistic regression, random forest (RF), support vector machines (SVM) or any deep learning models like RNN, LSTM or state-of-art model like BERT, GPT3 to predict the label. As a measure of accuracy ROC, Recall, F1-score can be used based the problem statement in hand.\n",
        "\n",
        "Before running the model, let's split the data into train and test.\n",
        "\n",
        "Now, model the data and check accuracy metrics.\n",
        "\n",
        "Apart from classification problems, NLP can be leveraged for several use cases like text summarization, Q&A, topic modeling (link), text translation etc."
      ],
      "metadata": {
        "id": "uMIDFxucaD-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Sentiment Analysis?**\n",
        "Sentiment analysis is a popular task in natural language processing. The goal of sentiment analysis is to classify the text based on the mood or mentality expressed in the text, which can be positive negative, or neutral.\n",
        "\n",
        "Sentiment analysis is the process of classifying whether a block of text is positive, negative, or neutral. The goal which Sentiment analysis tries to gain is to be analyzed people’s opinions in a way that can help businesses expand. It focuses not only on polarity (positive, negative & neutral) but also on emotions (happy, sad, angry, etc.). It uses various Natural Language Processing algorithms such as Rule-based, Automatic, and Hybrid."
      ],
      "metadata": {
        "id": "zGsdMca7d5WL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NB_3B8j_eBGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advantages of NLP**\n",
        "1. NLP helps us to analyse data from both structured and unstructured sources.\n",
        "2. NLP is very fast and time efficient.\n",
        "3. NLP offers end-to-end exact answers to the question. So, It saves time that going to consume unnecessary and unwanted information.\n",
        "4. NLP offers users to ask questions about any subject and give a direct response within milliseconds."
      ],
      "metadata": {
        "id": "ivjxaLvsbgg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Disadvantages of NLP**\n",
        "1. For the training of the NLP model, A lot of data and computation are required.\n",
        "2. Many issues arise for NLP when dealing with informal expressions, idioms, and cultural jargon.\n",
        "3. NLP results are sometimes not to be accurate, and accuracy is directly proportional to the accuracy of data.\n",
        "4. NLP is designed for a single, narrow job since it cannot adapt to new domains and has a limited function."
      ],
      "metadata": {
        "id": "eT3Kw4utbuW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Components of NLP**\n",
        "There are two components of Natural Language Processing:\n",
        "\n",
        "1. Natural Language Understanding\n",
        "2. Natural Language Generation"
      ],
      "metadata": {
        "id": "P7Nb8EUdb0of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Applications of NLP**\n",
        "The applications of Natural Language Processing are as follows:\n",
        "\n",
        "1. Text and speech processing like-Voice assistants – Alexa, Siri, etc.\n",
        "2. Text classification like Grammarly, Microsoft Word, and Google Docs\n",
        "3. Information extraction like-Search engines like DuckDuckGo, Google\n",
        "4. Chatbot and Question Answering like:- website bots\n",
        "5. Language Translation like:- Google Translate\n",
        "6. Text summarization"
      ],
      "metadata": {
        "id": "dYzFKQtfb_gT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Libraries**\n",
        "1. NLTK\n",
        "2. Spacy\n",
        "3. Gensim\n",
        "4. fastText\n",
        "5. Stanford toolkit (Glove)\n",
        "6. Apache OpenNLP"
      ],
      "metadata": {
        "id": "x2Z2Zvl6corW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Questions**\n",
        "### **What is the most difficult part of natural language processing?**\n",
        "Ambiguity is the main challenge of natural language processing because in natural language, words are unique, but they have different meanings depending upon the context which causes ambiguity on lexical, syntactic, and semantic levels.\n",
        "\n",
        "### **What are the 4 pillars of NLP?**\n",
        "The four main pillars of NLP are\n",
        "1. Outcomes,\n",
        "2. Sensory acuity\n",
        "3. behavioural flexibility\n",
        "4. report\n",
        "\n",
        "### **What language is best for natural language processing?**\n",
        "Python is considered the best programming language for NLP because of their numerous libraries, simple syntax, and ability to easily integrate with other programming languages.\n",
        "\n",
        "### **What is the life cycle of NLP?**\n",
        "There are four stages included in the life cycle of NLP\n",
        "– development, validation, deployment, and monitoring of the models."
      ],
      "metadata": {
        "id": "V-YctktocyCa"
      }
    }
  ]
}